#+CATEGORY: curro, villanueva, periodismodatos
#+TAGS: tabula, ocr, google. drive, scraping, kimono
#+DESCRIPTION: Herramientas de scraping de PDF y Web
#+AUTHOR: Adolfo Antón Bravo
#+EMAIL: adolflow@gmail.com
#+TITLE: Scraping: PDF y Web
#+DATE: [2015-12-16 mié 16:00]

#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \addto\captionsenglish{\renewcommand{\contentsname}{{\'I}ndice}}
#+OPTIONS: ^:nil num:nil

#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1200 reveal_height:800
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANS: linear
#+REVEAL_THEME: sky
#+REVEAL_HLEVEL: 2
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Herramientas de Scraping de PDF y Web.">
#+REVEAL_POSTAMBLE: <p> Creado por adolflow. </p>
#+REVEAL_PLUGINS: (highlight notes)
#+REVEAL_EXTRA_CSS: file:///home/flow/Documentos/software/reveal.js/css/reveal.css
#+REVEAL_ROOT: file:///home/flow/Documentos/software/reveal.js/

Módulo de periodismo de datos de /scraping/ donde aprenderemos cosas directamente relacionadas:

- Web /scraping/
- PDF /scraping/
- Nociones de programación
- Detectar y estructurar información localizada en web o en otros formatos para poder analizarla e interpretarla.
- Volcar y formatear datos de diferentes orígenes para poder gestionarlos.

Pero también otras cuestiones directamente relacionadas como:

- Aprender a instalarnos programas adecuadamente
- Resolución de problemas
- Manejo de versiones
- Github
- Tecnologías web
- Empezar a practicar con lenguajes de programación

* Necesidades técnicas

- Navegadores [[http://getfirefox.com/][Firefox]] (o derivados) y [[https://www.google.es/chrome/browser/desktop/][Chrome]] (o derivados)
- Editor de textos con indentado y resaltado de sintaxis, [[http://infotics.es/2015/11/11/editor-de-textos/][elige el que más te guste]]
- Extensiones del navegador: [[http://getfirebug.com/][Firebug]], [[https://addons.mozilla.org/En-uS/firefox/addon/xpath-checker/][Xpath Checker]], [[http://www.tineye.com][Tineye]]
- Cuenta Google Drive.
- Programas de copia, descarga, spiders o crawling como [[https://www.httrack.com/][httrack]] o [[http://www.downthemall.net/][Downthemall]].
- Programas de web scraping como [[https://www.outwit.com/][Outwit]], [[http://import.io][Import.io]] y [[http://kimonolabs.com][kimono]].
- Programas de tratamiento de PDF como [[http://www.foolabs.com/xpdf/][xpdf]].
- La terminal, también es posible desde Windows con [[https://www.cygwin.com/][cygwin]]
- Python, Ruby y R


* La Web como fuente de datos

- La web es un fuente de datos inagotable
- De hecho, se puede considerar una gran base de datos.
- Se puede acceder a servicios web anónimos, con perfiles, buscadores generales y específicos.
- El buscador es un software que recopila e indexa archivos almacenados en servidores web y recupera la información conforme a algunos criterios específicos.
** Recopilar datos
 - La manera más sencilla de extraer datos de una página web consiste en seleccionar el texto, copiarlo y pegarlo allí donde queremos luego trabajar con ello.
 - Si se tratan de datos que se encuentran dispuestos en tablas, los podemos copiar y pegar en nuestra aplicación de hoja de cálculo favorito.
 - Es la forma más fácil pero también *manual*.
 - Si queremos *automatizar* esa tarea, o incluso *si no nos deja hacerlo manualmente*, tenemos que optar por otras opciones.
 - Las opciones más fáciles pasan por que los datos estén bien estructurados, un HTML bien formado
 - Se habla de recopilación de datos, /crawling/ o /spiding/ cuando nos descargamos sitios completos.
 - Se habla de /scraping/ cuando rascas de aquí y de allá.

** Búsqueda avanzada

- Se puede utilizar búsqueda avanzada en los principales buscadores
- /Google/, /Yahoo!/, /duckduckgo/, /Yandex/ o /Bing/
- También están los [[https://support.google.com/websearch/answer/2466433][operadores de búsqueda]]: permiten filtrar las búsquedas.
- Un listado no oficial lo encontrais en [[http://www.googleguide.com/advanced_operators_reference.html][googleguide]]

** Ejemplo con operador =site:= de Google

- Los operadores de búsqueda nos permiten acotar y especificar más los resultados.
- Se puede buscar una palabra específica en un dominio concreto con =site:dominio=
- Por ejemplo, busquemos la palabra *inmigración* en el sitio de [[https://www.google.es/?gfe_rd=cr&ei=QVl-VoOmOtGp8weIvqSwBg#q=site:elpais.com+inmigraci%25C3%25B3n][elpais.com]]
- El resultado no varía en función de nuestro perfil o nuestra navegación ya que es sobre un sitio concreto.
- Sí que puede variar desde que he realizado la búsqueda hasta que lo buscáis.

La búsqueda devuelve 55200 resultados.

#+BEGIN_SRC google
site:elpais.com inmigración
#+END_SRC

** Operador lógico /AND/

- Cuando buscamos una palabra buscamos unos caracteres escritos de esa manera determinada.
- Si buscamos más de una palabra, comienzan a actuar operadores lógicos.
- Los operadores se utilizan para afinar aun más la búsqueda.
- El operador por defecto cuando se buscan dos palabras es es =+=, =AND= o =Y=.
- En Google se utiliza =+=, no los otros.
- Eso quiere decir que se buscan documentos que contengan esas dos palabras.

La búsqueda de =inmigración ilegal= devuelve los mismos resultados, 6680, que =inmigración + ilegal=

#+BEGIN_SRC google
site:elpais.com inmigración ilegal
#+END_SRC

** Operador lógico /OR/
- Otro operador muy utilizado es =OR=, =O= o =|=
- En Google se utilizan =OR= y =|=
- Busca documentos donde aparecen una u otra palabra.

Si buscamos =inmigración OR ilegal= da los mismos resultados que si buscamos =inmigración | ilegal=, 16700 resultados:

#+BEGIN_SRC google
site:elpais.com inmigración OR ilegal
#+END_SRC

** Palabras reservadas

- Los operadores lógicos nos introducen en el concepto de las /palabras reservadas/.
- Las palabras reservadas se dan en todos los lenguajes informáticos y son palabras que utiliza el propio lenguaje.
- Son palabras o caracteres reservados =AND=, =OR=, =+=, =-=, =|=, etc.
- Lo cual nos lleva al concepto de /literales/
- Si queremos utilizar esas palabras reservadas como palabra a buscar, deberemos entrecomillarlas.

Lo cual buscará documentos donde aparezcan las tres palabras: =spain=, =or= y =greece=

#+BEGIN_SRC google
site:theguardian.com spain "or" greece
#+END_SRC


** Búsqueda literal

- La búsqueda literal sirve para encontrar expresiones específicas.
- Los literales se denominan también en programación /strings/ o /cadenas de caracteres/.
- Por ejemplo, la búsqueda de =violencia de género en madrid= encuentra 330 resultados porque hay 330 documentos donde aparece esa expresión tal cual.

Si queremos que aparezca =violencia de género= y también =madrid=, pero sin estar juntos en la expresión, devuelve 20200 resultados.

#+BEGIN_SRC google
site:elpais.com "violencia de género" madrid
#+END_SRC

** Operador menos
- Si el operador =+= se utiliza como sinónimo de =OR=, el operador menos reduce los resultados de la búsqueda.
- Con el operador =-= elegimos palabras que no queremos que aparezcan acompañadas de otras.
- Observad que el signo de menos debe estar pegado al texto.
- Seguimos con el ejemplo anterior y *concatenamos operadores*
- Cuántas veces aparece "violencia de género" pero no aparece =2014=, =2011= y =2010=, para que no aparezcan las noticias de esos años, aparecen 9 resultados.

Incluso se combina con el operador =site:=:

#+BEGIN_SRC google
"violencia de género en españa" -site:wikipedia.org -2014 -2011 -2010
#+END_SRC

** Operador comodín
- El asterisco encuentra cualquier cosa en medio de una cadena de caracteres.
- Si queremos buscar /violencia de género/, /violencia a las mujeres/ y /violencia contra las mujeres/, podemos emplear el operador el operador =|=

Y apoyarnos en el operador comodín =*=

#+BEGIN_SRC google
site:elpais.com "violencia * mujeres|género"
#+END_SRC


** Debemos saber que

- Las búsquedas en estos buscadores no son /case sensitive/, es decir, no distinguen entre minúsculas y mayúsculas.
- Si buscamos "violencia de género" obtendremos los mismo resultados que si buscamos "VIOLENCIA DE GÉNERO"
- el uso de estos operadores implica el uso de *expresiones o palabras reservadas*
- Las palabras reservadas son las que el buscador entiende como propias de su vocabulario.
- Para utilizar palabras reservadas en la búsqueda debemos entrecomillarlas.
- Las comillas también se utilizan para buscar literales, expresiones exactas.

** Resumen operadores lógicos

- El operador de restricción =+=, es similar a =AND=
- El operador de restricción =-=, es similar a =NOT= en otros tipos de búsqueda.
- El operador =OR=, donde esté cualquiera de los dos términos, o =|=
- =""=, se entrecomilla el texto exacto que queremos buscar, búsqueda literal.
- Si no queremos aprender estas reglas, podemos usar el buscador avanzado de Google que ofrece una interfaz gráfica en forma de formulario.
http://www.google.com/advanced_search



** Operadores de tipos de archivo
- Otro operador de búsqueda de google es el operador de tipos de archivo.
- Se puede buscar documentos que estén en un determinado formato de archivo en un sitio determinado
- O se pueden buscar palabras que estén en un documento con un formato determinado.
- O concatenar aun más con el operador de sitio y el de tipo de archivo.

*** Búsqueda en sitio concreto y tipo de archivo concreto

Busca documentos que estén en formato =xls= en =elpais.com=

#+BEGIN_SRC google
filetype:xls site:elpais.com
#+END_SRC

*** Búsqueda de literales en tipo de archivo concreto

Busca literales =violencia de género= que estén en un documento con un formato =csv=

#+BEGIN_SRC google
filetype:csv "violencia de género"
#+END_SRC

*** Búsqueda de literales en tipo de archivo en sitio concreto

Busca literales =violencia de género= en archivos =csv= en el sitio del =ine.es=

#+BEGIN_SRC google
filetype:csv site:ine.es "violencia de género"
#+END_SRC

** Operadores de búsqueda
- El listado completo de operadores de búsqueda disponible se encuentra en [[https://support.google.com/websearch/answer/2466433?hl=en][Google]].

*** =link:=
=link:= encuentra páginas que enlazan a cierta página.

#+BEGIN_SRC google
link:okfn.es
#+END_SRC

*** =related:=
=related:= encuentra páginas similares a una de tu elección.

#+BEGIN_SRC google
related:okfn.es
#+END_SRC

*** =info:=
=info:= obtiene información sobre una página web, incluida la versión cacheada de la página, páginas similares y páginas que enlazan con el sitio.

#+BEGIN_SRC google
info:okfn.es
#+END_SRC

*** =cache:=

=cache:= muestra la página la última vez que Google visitó la página.

#+BEGIN_SRC google
cache:okfn.es
#+END_SRC

** Desafíos
- Busca patrones en la información, los datos o la estructura de las páginas:

- Estructura de páginas estructurada:
http://www.ejercito.mde.es/unidades/Cordoba/index.html 
- Una tabla en una web
https://en.wikipedia.org/wiki/List_of_Spanish_provinces_by_sequence_or_length_of_coastline
- Tablas páginadas con /URLs/ distintas
http://www.bbc.co.uk/food/recipes/

* Otras utilidades
Veremos algunas utilidades que nos pueden ayudar de una u otra manera:
- /Bitly/
- /Twitter/
- /Archive.org/
- /Wayback Machine/
- Código fuente
- /TinEye/
- Otras: /Readability/, /Downthemall/.
** Bit.ly
- El servicio de /urls/ cortas nos puede ayudar a saber cuántas veces se ha compartido un determinado enlace.
- Tenemos que ir a http://bit.ly e introducir la /URL/ que nos interesa.
- Si ya se ha utilizado, aparecerá una /URL/ corta y podremos ver esa información escribiendo la URL seguida del símbolo =+=.
- Así vemos las estadísticas completas de la página, da una idea de lo popular que era la página y de lo que fue utilizado por las redes sociales como /Twitter/ o /Facebook/.
** Twitter
- Con /Twitter/ hacemos algo parecido a /Bit.ly/
- No vamos a poder disfrutar del mismo nivel de estadísticas.
- Se trata de buscar en el buscador de Twitter un enlace que nos interese y mostrará las últimas veces que se ha compartido.
- No guarda un histórico de todo el tiempo.
** La máquina del tiempo de archive.org
- La herramientas [[http://archive.org/web/web.php][Wayback Machine]] de [[http://www.archive.org][Internet Archive]] guarda pantallazos periódicos de las webs
- 456 mil millones de páginas en total que puedes consultar desde su buscador.
- Una vez encontrada esa página, ese dominio, podemos ver en una línea de tiempo los distintos instantes que ha guardado la máquina y ver qué aspecto tenía.
- No guarda muchas imágenes o estilos, por cuestiones de espacio, pero nos puede dar alguna sorpresa.

** Código fuente HTML
- A menudo en el código fuente los programadores han realizado comentarios o han ocultado algo que podía estar en otro momento y que ahora no conviene.
- No hace falta saber de /HTML/ pero sí que hay que saber dónde buscar.
- Los comentarios, contenido que el navegador no muestra, se encuentran entre los signos de =<!--= apertura de comentario y =-->= cierre de comentario.
- Por ejemplo, en la página de [[http://theguardian.co.uk][The Guardian]] ahora ponen que están contratando programadores.

** TinEye
- TinEye permite controlar el uso de las imágenes en un sitio web
- Nos pueda dar pistas sobre el origen, la fuente, otros usos de esa imagen...
- Se puede subir una imagen o bien escribir una /URL/, ya sea de la imagen o de la página web que contiene esa imagen
- Es capaz de mostrar unos resultados que pueden ser curiosos.. Por ejemplo, cuando se produce un acontecimiento importante, se suelen utilizar las mismas imágenes para ilustrarlo.
- Con TinEye podemos ver en qué medios se están utilizando esas imágenes y cuándo lo hicieron, por lo que podemos saber quién lo hizo primero y crear una línea temporal del uso de la imagen.
- Tiene extensiones para Firefox, Opera, Chrome o Safari. http://www.tineye.com
- Ejemplo: https://www.tineye.com/search/1dc12635c9e2e21a53002ef0ce9ac0e458d59492/

** Otras

- [[http://www.readability.com][Readability]], servicio web que ayuda a extraer texto de la página web. Dispone de extensión para Firefox.
- [[http://www.downthemall.net/][DownThemAll]], extensión que permite la descarga de varios archivos a la vez.
* Outwit Hub
- Outwit Hub, software y extensión para el navegador que contiene varias utilidades de reconocimiento y extracción de contenidos web y de organizar las colecciones de datos.
- Busca automáticamente a través de páginas.
- La primera vista en el marco de la izquierda es =Página=, mientras que los otros objetos son: =links=, =images=, =data=
- OutWit considera la página web como elementos de datos, por lo que si nos ponemos sobre el objeto =images=, seleccionaremos todas las imágenes.
** Conceptos Outwit
Hay tres conceptos en OutWit Hub:

 1. Una cesta de la compra llamada /catch/ o la /cesta/, discpuesto al pie de la página para recoger todo lo que queramos.
 2. Podemos filtrar la información por cada tipo de datos y recogerla en nuestra cesta.
 3. Puedes navegar a lo largo de varias páginas con el botón de paginación.
** Vistas Outwit
Las vistas que muestra /OutWit/ son:

- /Página/, actual. Muestra imágenes, enlaces, correos electrónicos, textos, rss, enlaces de noticias y otros datos que pueden extraerse de la página.
- /Imágenes/, que aparecen en la página actual. Se pueden filtrar, ordenar y copiar en la cesta.
- /Enlaces/, que aparecen en la página actual. Se pueden filtrar, ordenar y copiar en la cesta.
- /Correos electrónicos/, que aparecen en la página. ídem.
- /Texto/, muestra el texto de la página.
- /RSS/, en caso de que los hubiera.
- /Tablas/, extrae el contenido de la tabla y se le puede aqplicar las operaciones típicas.
- /Listas/, extrae el contenido de las listas.
- /Scraper/, aplica un escrapeador previamente cargado a la página.
- /Source/, muestra el HTML de la página.

** Filtros Outwit
Los filtros de control de los que dispone:

- Ocultar local, si activas esta caja de control, la vista solo mostrará los enlaces salientes o las imágenes externas, y se ocultaán los elementos locales.
- Ocultar caché, cuando esta casilla esté activa, las URLs cacheadas no se mostrarán.
- Documentos, cuando esta casilla esté marcada, sólo se mostrarán las URLs que correspondan con docuementos: pdf, doc, xls, etc.
- /Script/, si está marcada esta casilla, se mostrarán las imágenes que vengan de scripts y el resto se ocultarán.
- /Style/, si se marca se ocultarán las imágenes que no vengan de CSS.
- Background, muestra las imágenes utilizadas como fondo si está marcada.

** Navegación Outwit
Cuando hay más de una página que cargar, cuenta con algunas opciones:
- /Next/, carga la página siguiente de la serie
- /Browse/, busca automáticamente a través de todas las páginas de una serie.
- /Dig/, puede explorar todos los enlaces de la página. Si pinchamos en /Dig/, en el menú podemos establecer la profundidad de la búsqueda. Si marcamos =depth \= 0=, buscará por todos los enlaces de la página pero si modificamos el valor a =1=, explorará también todos los enlaces de las páginas visitadas.
- /Site Home/, carga la página principal del sitio.
- /Slideshow/, muestra las imágenes de la página como un carrusel.

** Resultados
- Los datos se pueden exportar a CSV, TSV, HTML, XLS o crear scripts SQL para guardarlos en bases de datos.
- En la versión de pago también se pueden programar tareas.
- Se suele adoptar como estándar de tiempo entre petición y petición la de 2 segundos de retraso.
- Si nos encontramos con contenido generado dinámicamente a través de javascript, será mejor que recopilemos los datos manualmente o que aprendemos otra técnica.

** Enlaces Outwit
- Extensión, http://www.outwit.com/products/hub/license.php
- Add-on Firefox, https://addons.mozilla.org/en-US/firefox/addon/outwit-hub/
- Vídeo, https://www.youtube.com/watch?v=ffoXpBlHZpo

* Import.io
- Import.io es una herramienta para la extracción de datos de páginas web.
- No se necesita ningún aprendizaje de lenguajes de programación.
- Para probarla podemos hacerlo a través de https://magic.import.io
- Podemos [[http://support.import.io/knowledgebase/articles/190281-how-do-i-install-import-io][descargárnosla de la web]]
- En http://magic.import.io pones la /URL/ que deseas probar y pulsas el botón "GET DATA".
- Nos aparecen los datos tabulados en una tabla.
- Eliminamos columnas que no queramos pinchando en la =(x)= que aparece.
- Si queremos guardar la información que aparece, tendremos que dar a =Extract Data=
- En la parte de abajo de la página salen tres botones.
 - Uno es por si lo que ha encontrado import.io no es lo que queríamos.
 - Si queremos descargarnos los datos para jugar con ellos, /Download CSV/
 - Pero también podemos crear una /API/, si estamos registrados.

* Kimonolabs
- Herramienta de /scraping/ que convierte web en API.
- Se utiliza como [[https://chrome.google.com/webstore/detail/kimono/deoaddaobnieaecelinfdllcgdehimih?hl%3Des%0A][extensión]] en Chrome/Chromium o atajo a los marcadores en Firefox.
- Una vez que estamos registrados en Kimono, podremos comenzar a crear nuestra propia API de cualquier sitio web apuntando al botón de Kimono.
- Una de las características de Kimono y que lo hacen muy atractivo es el hecho de crear una API sobre la base de cualquier web.
- La /API/ nos ofrece una forma de interactuar con el contenido de esa web de forma automática, por lo que podemos crear nuestra propia web o aplicación con nuestro propio /HTML/, /CSS/ y/o /JavaScript/ y poner el contenido de esa /API/, normalmente en formato /JSON/.
** Codesamples  

*** Curl
#+BEGIN_SRC sh
curl --include --request GET "https://www.kimonolabs.com/api/5gqzg2ws?apikey=NQEaiInc5MYvqqdD14pvzpHvpwkqsDQ3"

#+END_SRC
*** R

#+BEGIN_SRC R
library('bitops')
library('RCurl')
install.packages('rjson')
library('rjson')
json <- getURL('https://www.kimonolabs.com/api/5gqzg2ws?apikey=NQEaiInc5MYvqqdD14pvzpHvpwkqsDQ3')
obj <- fromJSON(json)
print(obj)



#+END_SRC
* wget
Permite descargar ficheros de sitios
* httrack
* Inspección técnica de HTML

- /Firebug/ se ha convertido en la herramienta estándar
- Chinche de la malva o zapatero, una extensión del navegador que permite analizar y descubrir todo lo necesario sobre la página web que estamos visitando para su modificación o interpretación.
- Comenzó como una extensión muy utilizada por desarrolladores web para editar, mostrar errores y monitorear el funcionamiento de /JavaScript/, /CSS/ y /HTML/ en tiempo real y en cualquier página.
- Permite también explorar el /DOM/ (/Document Object Model/ o modelo de objetos del documento) para crear selectores /CSS/ en conjunción con /JavaScript/ y así conseguir webs dinámicas.
- Tanto es así que Mozilla Firefox tiene una versión integrada de Firebug, en analizador.
- Si queremos experimentar un poco más, podemos descargar la extensión desde http://www.getfirebug.com

** Firebug
Una vez que hemos instalado la extesión de Firefox, podemos utilizarlo de dos manera:

- Atajos de teclado:
 - =F12= abre y cierra Firebug
 - =CTRL + F12= abre Firebug en una ventana nueva
 - =CTRL + F12=, si lo tienes abierto en ventana nueva, lo colca integrado
 - =SHIFT + F12=, cierra la consola si la tienes abierta
- Ratón:
 - Activamos la aplicación con el icono.
 - También podemos seleccionar cualquier parte de la web con el botón derecho y pinchar en =inspeccionar elemento=
   
** Consola
En ambos casos trabajamos con una consola que nos muestra la información de la página, en concreto:
- Consola, muestra información de errores del JavaScript de la página. Podemos introducir y ejecutar comandos.
- /HTML/, muestra el /HTML/ como nodos DOM de una jerarquía. Los nodos individuales pueden expandirse o contraerse para mostrar u ocultar los nodos hijos. También muestra el /box model/ (modelo de caja) CSS para cada elemento seleccionado.
- CSS, muestra todos los estilos CSS cargados y puedes introducir CSS y mofificarlos al momento. Hay una ventana que muestra todos los estilos cargados por nombre.
- Script, muestra los archivos JS incluidos y se pueden inspeccionar y activar o desactivar por partes.
- DOM, Muestra los objetos y propiedades DOM.
- Net, puedes comprobar cuańto tarda cada recurso en cargar, muestra las cabeceras de peticiones y respuestas HTTP para cada recurso.

* Google Drive

- importHTML()
- importFeed()
- importXML()
- importURL()
- importData()
- importRange()
** ImportHTML()
- El método más fácil consiste en importar datos de una tabla o una lista a Google Drive con la función =IMPORTHTML=
- Para ello tendremos que tener ciertas nociones de /HTML/
- Permite importar contenido de tipos de elementos /HTML/, tablas y listados.
- Afecta a los elementos =table= (tabla), =ul=, =ol= y =dl= (listados).
 - =ul=, que corresponde a /unordered list/ o lista desordenada, la típica lista donde cada elemento aparece con un punto o un guión.
 - =ol=, que corresponde a /ordered list/ o lista ordenada, donde los elementos del listado aparecerán ordenados, bien numérica o alfabéticamente, por ejemplo.
 - =dl=, corresponde con /description list/, listas de descripciones
 - =table=, corresponde con una tabla de datos tabulados.
- Construiremos la función =IMPORTHTML= con la /url/ entrecomillada, separado por punto y coma y entrecomillado el elemento del que queremos sacar la información, bien una lista =list= o una tabla =table=, seguido del número de elemento en la página de su mismo tipo, separado por otro punto y coma:

#+BEGIN_SRC google
=IMPORTHTML("URL";"list|table";n)
#+END_SRC

** ImportXML

- También podemos utilizar la función =IMPORTXML("url";"xpath_query")= para extraer otro tipo de información o acceder al contenido por =XPath=.
- Para ayudarnos a ello, además de /Firebug/, podemos utilizar la extensión de Firefox [[https://addons.mozilla.org/en-US/firefox/addon/xpath-checker/][XPath Checker]]
 -Por ejemplo, si queremos obtener el listado de todos los atributos =href= que contiene el elemento =a= que corresponde a los enlaces, de la /URL/, haremos:

#+BEGIN_SRC google
=IMPORTXML("URL";"//a/@href")

#+END_SRC

- Pero podríamos elegir sólo los enlaces que tienen una determinada clase, lo que haríamos también con /XPath/ de esta manera:

#+BEGIN_SRC google
=IMPORTXML("URL";"//a[@class='clase']")

#+END_SRC

- En vez de editar la fórmula completa, se puede poner la /URL/ en una celda, el elemento /XPath/ a buscar en otra y construir la expresión llamando a las celdas:

#+BEGIN_SRC google
=IMPORTXML(celda1;celda2)
#+END_SRC

- La potencia de /Xpath/ es /infinita/ y podemos hacer extracciones de datos muy concretas, como por ejemplo seleccionar solo los elementos que comiencen con una clase específica, como =[starts-with= y luego especificar la clase con el atributo =@= donde =class= es el valor del atributo =(@class, 'clase')=
- Si queremos sacar todos los enlaces una /URL/, después de inspeccionar la página, comprobamos que los enlaces se encuentran en un =div= que tiene la clase =clase=. Construimos esta fórmula de =IMPORTXML=

#+BEGIN_SRC google
=IMPORTXML("URL"; "//div[starts-with(@class,'clase')]")
#+END_SRC

Si quisiéramos los enlaces, añadiríamos al final =//@href=, ya que el enlace se encuentra en el atributo de =a=, =href=

#+BEGIN_SRC google
=IMPORTXML("URL"; "//div[starts-with(@class,'clase')]//@href")

#+END_SRC

Puede ser que la página no traiga los enlaces absolutos sino que sean relativos, por lo que podemos concatenarlos con la función =CONCATENATE=:

#+BEGIN_SRC google
=CONCATENATE("URL",celda-resultados)

#+END_SRC

Y luego estiramos esta función al resto de las celdas que lo requieran.

** Algunos ejemplos XPath útiles:
- =//=, descarga todos los elementos de html que empiecen con =<=
- =//a=, descarga todos los contenidos del elemento =a=, los enlaces, de la URL que decidamos.
- =//a/@href=, descarga todos los contenidos del atributo =href= del elemento =a=, que corresponden con la URL del enlace.
- =//input[@type='text']/..=, descarga todos los elementos padre de los elementos de texto =input=
- =count(//p)=, cuenta el número de elementos que le digamos, en este caso párrafos =p=
- =//a[contains(@href, 'protesta')]/@href=, encuentra todos los enlaces que contienen la palabra =protesta=
- =//div[not(@class='left')]=, encuentra todos los =div= cuyas clases no sean =left=
- =//img/@alt=, muestra todos los textos de los atributos =alt= de las imágenes =img=
** Ejemplo complejo

#+BEGIN_SRC google
=IMPORTXML("URL";"//div[@class='clase']//h3")
#+END_SRC

Nos da todos los titulares =h3= que se encuentran dentro del =div= con clase =clase=n de los artículos de la /URL/



* PDF
** Interroga a un /PDF/
Diagrama propuesto por Nicolas Kayser-Brill

file:///home/flow/Documentos/curro/unir/mineria-datos/temas/nkb-pdf.png


- ¿Es tu imagen un PDF?
 - Sí, entonces utiliza OCR para extraer datos
  - Google Drive, https://support.google.com/drive/answer/176692?hl=en
 - No, entonces ¿puedes copiar el texto seleccionándolo y pegándolo en otro documento?
  - No, tendrás que usar otro software
  - Sí, entonces puedes usar:
    - [[http://tabula.technology][Tabula]], para extraer datos automáticamente.
    - /PDFtoExcelOnline/: subes un PDF y te envían por correo el XLS. http://pdftoexcelonline.com
    - [[http://smallpdf.com][smallpdf]]

** Colaboración
- Si todo lo anterior falla, se puede utilizar alguna herramienta colaborativa.
- [[http://www.documentcloud.org][Documentcloud]], creada por miembxs de la comunidad de periodismo de datos.
- [[https://www.mturk.com/mturk/welcome][mTurk]] de /Amazon/.
- [[http://crowdcrafting.org/][Crowdcrafting]]

** Crowdcrafting: PDF Transcribe
- [[http://crowdcrafting.org/app/pdftranscribe/][PDF Transcribe]] es el nombre de la aplicación de /PyBossa/ que permite transcribir un /PDF/ colaborativamente.
- Utiliza la librería /Mozilla PDF.JS library/ para cargar un archivo /PDF/ externo y renderizarlo en la aplicación que se ejecuta en el navegador sin necesidad de terceros.
- A su lado se extiende un formulario  personalizado para extraer los datos que solicitamos de cada documento.
- Se pueden añadir tantos campos como sean necesarios en el formulario para recopilar la información que necesitemos, explicándolo convenientemente.
- Podemos asignar tareas a usuarios y completar de manera distribuida y colaborativa el proyecto de transcripción.

http://img10.imageshack.us/img10/5364/pdftranscribe1.png
#+CAPTION: Ventana de la app de transcripción de PDF con Crowdcrafting

** Probar, probar, probar

- ProPublica advierte en su guía para convertir PDF en documentos de texto que ninguna solución va a ser completa por la propia solución y/o por los documentos PDF.
- Lo normal es combinar las técnicas y encontrar las opciones que más satisfagan.
- Siempre se necesita una revisión manual o dos del trabajo realizado por las herramientas.

** pdftotext, xpdf
- /pdftotext/ es una herramienta de código abierto
- Funciona en consola
- Convierte archivos /PDF/ a texto plano.
- Forma parte de /xpdf/, un conjunto de aplicaciones para trabajar con documentos PDF. También se incluye como parte de /Poppler/, un proyecto derivado de /Xpdf/.

Solo puede convertir un documento cada vez:

#+BEGIN_SRC sh
pdftotext archivo.pdf

#+END_SRC

http://www.foolabs.com/xpdf/download.html

- Para especificar la primera página a convertir:
#+BEGIN_SRC sh
pdftotext -f numero-primera-pagina-convertir

#+END_SRC

- Especifica la última página a convertir:
#+BEGIN_SRC sh
pdftotext -l numero-ultima-pagina-convertir

#+END_SRC

*** Opciones

- Especifica la resolución, en puntos por pulgada. El valor por defecto es 72.

#+BEGIN_SRC sh
pdftotext -r resolucion-en-PPP

#+END_SRC

- Especifica la coordenada x del área que selecciona desde la esquina superior izqda:
#+BEGIN_SRC sh
pdftotext -x coordenada-x

#+END_SRC
- Especifica coordenada y del área que selecciona desde la esquina superior izqda:

#+BEGIN_SRC sh
pdftotext -y coordenada-y

#+END_SRC

- Especifica la anchura (la =W= es por /width/) del área en pixels. El valor por defecto es 0:
#+BEGIN_SRC sh
$ pdftotext -W valor-ancho

#+END_SRC

- Especifica la altura (la =H= es por /height/) del área seleccionada en pixels. El valor por defecto es 0.
#+BEGIN_SRC sh
pdftotext -H valor-alto

#+END_SRC

- Si queremos conservar el aspecto tanto como se pueda, hay que apuntar la opción =-layout=. El valor por defecto es =undo=, que fuerza el texto a mostrarse en texto corrido.

#+BEGIN_SRC sh
pdftotext -layout

#+END_SRC

- Ancho fijo tabulado, con el valor de la anchura en puntos, lo cual fuerza el modo de disposición física:

#+BEGIN_SRC sh
pdftotext -fixed valor-ancho-en-puntos

#+END_SRC

- Generación de archivo /HTML/ que incluya la meta información, encaja el texto en elementos =<pre>= y =</pre>=
#+BEGIN_SRC sh
pdftotext -htmlmeta

#+END_SRC

- Con la opción =-bbox= se genera un archivo /XHTML/ que contiene caja de información de cada palabra.

#+BEGIN_SRC sh
pdftotext -bbox

#+END_SRC

- Si queremos especificar la codificación del formato de salida. Por defecto es /UTF-8/:

#+BEGIN_SRC sh
pdftotext enc codificacion

#+END_SRC

** Ghostscript

- Es el lenguaje que entienden las impresoras.
- Se puede utilizar para reducir el tamaño de archivos /PDF/
- Si tienes /Ghostscript/ instalado (=gs=), puedes correr este comando en consola:
#+BEGIN_SRC sh
gs -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/screen -sOutputFile=nuevo_archivo.pdf original.pdf
#+END_SRC

En las opciones de =-dPDFSETTINGS=, puedes optar por:
- =/screen=. si quieres una baja resolución y un tamaño pequeño.
- =/ebook=, selecciona resolución media y tamaño mediano.
- =/printer= y =/prepress=, para resoluciones altas

** PDFtk

- PDFtk permite realizar muchas operaciones con /PDF/
- Necesitas el comando =pdftk= desde la terminal.
- O bien o [[http://www.pdflabs.com/tools/pdftk-the-pdf-toolkit/][pdf toolkit]] http://www.pdflabs.com/tools/pdftk-the-pdf-toolkit/

*** Rotar documentos

- Para rotar por completo un /PDF/ o páginas determinadas, en los puntos cardinales elegidos.
- Por ejemplo, para rotar una página 90 grados en el sentido de las agujas del reloj:
#+BEGIN_SRC sh
pdftk entrada.pdf cat 1east output salida.pdf

#+END_SRC

- Donde =1east= significa que la página =1= gira 90º al =East= (Este)

Las opciones son:
- =north=, 0 grados
- =east= o =right=, 90º
- =south= o =down=, 180º
- =west= o =left=, 270º
=left=, =right= and =down= hacen ajustes relativos a la rotación de la páginas.

*** Dividir un PDF en varios

- Si queremos dividir (/split/) un /PDF/ muy largo en varios documentos /PDF/, podemos utilizar la opción =burst=
#+BEGIN_SRC sh
pdftk pdflargo.pdf burst
#+END_SRC

Lo que dará como resultado tantos archivos /PDF/ como páginas tenía el documento.

** ImageMagick

- /Imagemagick/ es un conjunto de herramientas en consola para modificar y tratar imágenes, bien en el momento o bien integrado en un /script/ en /bash/.
- Lo podemos utilizar para convertir imágenes en /PDF/ y así manipular el documento como /PDF/.
- Utiliza el comando =convert=

Convert:

#+BEGIN_SRC sh
convert *.jpg +adjoin page-%d.pdf
#+END_SRC
Donde =jpg= es el formato de la imagen, pero podríamos trabajar también con archivos =png= o =gif=.

Si no nos aparece tal como queremos, quizás podamos jugar con los parámetros:

#+BEGIN_SRC sh
convert -density 150 archivo.pdf -quality 90 salida.png prefijo

#+END_SRC

Donde:
- =-density= especifica el valor de la resolución en /PPP/.
- =-quality= especifica la calidad, donde 100 significa =sin compresión=
- =prefijo= indica el prefijo a utilizar al crear cada archivo de imagen.

También podríamos hacer la conversión en un nuevo directorio:
#+BEGIN_SRC sh
mkdir convertidas && for i in *.pdf; do convert -density 150 $i convertidas/$i; done
#+END_SRC

Con la opción =-colorspace Gray= convertimos una imagen de color a escala de grises.

** Poppler-utils pdftoppm

- Con las utilidades /poppler-utils/ manipulamos /PDF/
- Su paquete /pdftoppm/ permite convertir /PDF/ a imágenes en formato /ppm/, /png/ o /jpg/.

#+BEGIN_SRC sh
pdftoppm -png file.pdf prefijo
#+END_SRC

- Lo que produce tantos /png/ como páginas tiene, con el prefijo /prefix/.
- La resolución por defecto es 150 /ppp/ /puntos por pulgada/.

Para incrementar la resolución, se puede hacer con las opciones =rx= y =ry=.

#+BEGIN_SRC sh
pdftoppm -rx 300 -ry 300 -png archivo.pdf prefijo
#+END_SRC

Para imprimir solo una página, se utiliza la opción =singlefile=, y con la opción =n= se especifica el número de página. La primera página es la número 1.

#+BEGIN_SRC sh
pdftoppm -f N -singlefile -png archivo.pdf prefijo
#+END_SRC

** pdfcrop

https://sourceforge.net/projects/pdfcrop/files/
texlive-extra-utils

#+BEGIN_SRC sh
pdfcrop --margins '5 10 20 30' input.pdf output.pdf

#+END_SRC

Automatizar archivos =PDF= de un directorio:

#+BEGIN_SRC sh
,#!/bin/bash

for FILE in ./*.pdf; do
  pdfcrop --margins 5 "${FILE}" "${FILE-cropped}"
done

#+END_SRC

** pdfjam

Utilidad en línea de comandos para el paquete de LaTeX /pdfpages/

#+BEGIN_SRC sh
pdfjam --keepinfo --trim "15mm 15mm 30mm 60mm" --clip true --suffix "cropped" 6_Abengoa.pdf 

#+END_SRC
** PDF-Shuffler

Aplicación gráfica (GTK) para Linux que ayuda a pegar, dividir, rotar o reorganizar las páginas a través de una interfaz gráfica. Es un frontend para /python-pyPdf/

https://sourceforge.net/projects/pdfshuffler/

** Briss

Aplicación multiplataforma (/Linux/, /Windows/ y /Mac OSX/ para recortar archivos PDF. Una simple interaz nos permite definir la región.

http://briss.sourceforge.net/
https://sourceforge.net/projects/briss/files/

** qpdf

Para desencriptar un /PDF/ encriptado

#+BEGIN_SRC sh
qpdf --password='' --decrypt archivo-original.pdf archivo-desencriptado.pdf
#+END_SRC

** pyPdf

Receta de Python
http://code.activestate.com/recipes/576837-crop-pdf-file-with-pypdf/
python-pypdf2 - Pure-Python library built as a PDF toolkit (Python 2)
python3-pypdf2 - Pure-Python library built as a PDF toolkit (Python 3)
python-pypdf - PDF toolkit implemented solely in Python

** Rmagick

- [[http://rmagick.rubyforge.org/portfolio.html][Rmagick]] es otra herramienta que recomiendan en el artículo de ProPublica para trabajar con imágenes desde la consola
- También permite realizar dibujos en 2D.
- Transformaciones: http://rmagick.rubyforge.org/portfolio.html
- Dibujos: http://rmagick.rubyforge.org/portfolio3.html
- Efectos especiales: http://rmagick.rubyforge.org/portfolio2.html
- Repositorio en GitHub: http://github.com/rmagick/rmagick

** PDF Split and Merge

- Si nos da un poco de respeto o no podemos acceder a una línea de comandos, podemos contar con [[http://sourceforge.net/projects/pdfsam/%0A][PDF Split and Merge]]
- Se trata de una herramienta en modo gráfico (y también en línea de comandos) para separar, juntar, mezclar y rotar /PDF/.
- Está disponible para /Windows/, /MacOSX/ y /GNU/Linux/ y el único requisito es que necesita del entorno virtual de /Java/.
- Entre sus funcionalidades, destacan:
 - Pegar documentos /PDF/
 - Dividir documentos /PDF/, especificando el número de páginas
 - Dividir documentos /PDF/, especificando el nivel de los marcadores
 - Rotar /PDF/
 - Mezclar dos documentos /PDF/, componiendo uno nuevo que alterne una página de cada.
 - Componer visualmente un nuevo /PDF/ arrastrando páginas de otros /PDF/.

** Herramientas Web

- Zamzar, Cometdocs y Smallpdf son servicios web online.
- Por tanto, el documento deja de estar en tu equipo y pasa por Inet hasta sus servidores.
- Se suben los archivos ahí y o bien te devuelven una salida en un formato de texto o bien un correo electrónico con un enlace para que los puedas descargar.
- [[http://www.zamzar.com/][Zamzar]] es un servicio web para realizar conversiones de todo tipo.
- [[http://www.cometdocs.com/][Cometdocs]] es otro servicio web para realizar conversiones aunque también funciona como servicio de almacenamiento y para compartir archivos. Dispone de una versión escritorio para /Windows/ e /iOS/. También dispone de una API como servicio de pago. 
- [[http://smallpdf.com/][smallpdf]] es otro servicio web que permite manipular y convertir /PDF/

** Tika

[[http://okfnlabs.org][OKFN Labs]] tiene en fase beta un [[http://okfnlabs.org/blog/2015/02/21/documents-to-text.html][servicio web]] que permite convertir un gran número de tipos de archivo a =TXT=.

El servicio web se encuentra en http://beta.offenedaten.de:9998/tika, y para comprobar su funcionamiento tan solo hemos de tener una imagen que contenga texto y lanzar la consulta, en /Mac/, /GNU/Linux/ o /Cygwin/, desde la terminal:

#+BEGIN_SRC sh
curl -T ruta_archivo_imagen http://beta.offenedaten.de:9998/tika
#+END_SRC

En =STDOUT=, es decir, en la consola, se nos mostrará el texto que contiene la imagen.

Si queremos guardarlo automáticamente en una archivo de texto para facilitar su revisión, podemos añadir una concatenación de tareas:

#+BEGIN_SRC sh
curl -T ruta_archivo_imagen http://beta.offenedaten.de:9998/tika > ruta_archivo_a_revisar.txt
#+END_SRC

- El proyecto lo realizó Matt Fullerton de [[http://okfnlabs.org][OKFNLabs]] a partir del servidor web de /Apache/ /Tika Project/, que soporta /Tesseract/.
- Ha creado una imagen /docker/ por si quieres replicarlo en tu [[https://registry.hub.docker.com/u/mattfullerton/tika-tesseract-docker/%0A][propio servidor]]
- Y también puedes construirla desde [[https://github.com/mattfullerton/tika-tesseract-docker][GitHub]]

** Good Tables

- /Good Tables/ es otro proyecto de [[http://okfnlabs.org][OKFNLabs]] que consiste en un paquete /Python/ para validar datos tabulares.
- Puede funcionar con simples /CSV/ o en una tubería de procesos /ETL/.
- Se trata de una versión alfa
- Artículo sobre Good Tables, http://okfnlabs.org/blog/2015/02/20/introducing-goodtables.html
- Read the Docs, https://goodtables.readthedocs.org/en/latest/

* Un caso singular: ProPublica
- [[http://propublica.org/][ProPublica]] realiza grandes investigaciones de periodismo de datos.
- Y tanto o más importante: lo documentan.
- Un ejemplo concreto lo tuvieron con el proyecto [[https://projects.propublica.org/docdollars/][Dollars for Docs]] y cómo utilizaron [[http://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick][varias herramientas]]
- En este caso crearon la guía /Scraping for Journalism, A guide for Collecting Data/
 - [[https://www.propublica.org/nerds/item/doc-dollars-guides-collecting-the-data][Recolección de datos]]
 - [[http://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick][Tratamiento de imágenes]]
 - [[http://www.propublica.org/nerds/item/turning-pdfs-to-text-doc-dollars-guide][PDF scraping]]

** El proceso
Ante una tabla de datos en una imagen:
1. En un programa de tratamiento de imágenes, con líneas guía, crearon una cuadrícula donde el texto de cada celda pudiera posteriormente seleccionarse con la herramienta de selección rectangular.
2. Dividieron la imagen y crearon imágenes nuevas, una por cada celda con texto, identificándolas apropiadamente con el número de celda por fila (/row/) y por columna (/column/) correcto.
3. Para ello utilizaron distintas operaciones de /RMagick/.
4. Descartaron todo el espacio en blanco de los márgenes de la tabla con [[http://studio.imagemagick.org/RMagick/doc/image1.html#bounding_box][bounding_box]], 
5. Convirtieron la tabla a blanco y negro (escala de grises) para que el /OCR/ funcione mejor.
6. Primero probaron con las opciones de /RMagick/ =quantize= y =contrast=, pero como los resultados no fueron los esperados lo hicieron con /Photoshop/, que permite también operaciones por lotes.
7. Crearon una imagen con cada celda con un nombre que lo identificara su posición en la tabla por número de fila y número de columna. Así lograron 500 archivos de imagen.  
8. Detectaron las líneas de la tabla para crear coordenadas de celdas con la opción =get_pixels=, sobre las que cortar la imagen de la tabla en imágenes de cada celda.
9. Realiza /OCR/ con /tesseract/, integrado en el /script/ con esta llamada (más adelante se ve el /script/ completo donde se integra esta llamada):

#+BEGIN_SRC ruby
`tesseract /cell-files/#{j}x#{i}.tif /cell-files/#{j}x#{i}.txt `
#+END_SRC

10. Construye la tabla de datos en modo texto con el texto resultante del paso anterior, también de manera automatizada incorporándolo al /script/:

#+BEGIN_SRC ruby
,# abre el archivo que tesseract ha creado y almacena el texto en el array
	       text_row << File.open("cell-files/#{j}x#{i}.txt", 'r').readlines.map{|line| line.strip}.join(" ")

	     end

\# une el array con los caracteres de tabulación y produce una salida de datos en una línea
	     output_file.puts( text_row.join("\t"))

#+END_SRC

11. Limpieza de los datos, ya que /Tesseract/ puede equivocarse en caracteres similares, como por ejemplo =0= de cero y =O= de o mayúscula. Por lo que probablemente no podamos dejar de hacer una comprobación manual, si bien en este proceso podemos implicar a más personas, bien a través de las citadas /Crowdcrafting/ o /mTurk/. En este sentido, ProPublica utilizó /mTurk/ y realizó esta [[http://www.propublica.org/article/propublicas-guide-to-mechanical-turk][guía]].


** PDFTables
Servicio web creado por ScraperWiki, un servicio de /web scraping/, para extraer tablas de PDFs. Requiere crearse un usuario. Puedes descargar los resultados a través del navegador. Advierten que si usas muchos documentos, tendrás que convertirte en usuario de pago.
https://pdftables.com/

* OCR

El reconocimiento óptico de caracteres /OCR/ (por sus siglas en inglés /Optical Character Recognition/) nos permite convertir imágenes que contienen texto en documentos de texto gracias a algoritmos automáticos que realizan ese reconocimiento.

** Google Drive

- Google Drive realiza OCR sobre imágenes individuales en formato /jpg/, /png/ o /gif/ pero también en documentos /PDF/ de una o más páginas
- Google recomienda ciertas pautas para el uso de OCR:
 - Los archivos han de tener la más alta resolución, ya que así funcionará mejor OCR. Como medida de ejemplo, recomiendan que cada línea de texto sea de al menos diez píxeles de altura.
 - Es importante que estén orientados en horizontal de izquierda a derecha. Si no lo tienes así, utiliza las herramientas citadas anteriormente para resolverlo.
 - Los idiomas y conjuntos de caracteres que soporta con seguridad son /Latin/, el soporte de otros es experimental. Puedes elegir el idioma de los documentos en el menú.
 - Reconocen mejores resultados en fuentes comunes como /Arial/ o /Times New Roman/.
 - La calidad de la imagen también es importante. Las imágenes con mayor contraste funcionan mejor, las que están movidas o borrosas peor.
 - El tamaño máximo para las imágenes es de 2 MB por imagen.
-  En los documentos /PDF/ solo trabajará con las 10 primeras páginas, por lo que conviene dividir los documentos.
- /Google OCR/ pretende mantener el formato básico del texto, como son las negritas o las itálicas, el tamaño y el tipo de fuente y los saltos de línea, pero reconocen que detectar estos elementos es complicado y no siempre lo consiguen.
- Otros contenidos estructurados como listas numeradas, listados estructurados, tablas, columnas de texto, pies de página y notas finales es probable que no sean reconocidos.

** Tesseract-ocr
- [[https://github.com/tesseract-ocr][Tesseract]] se anuncia como las más completa y precisa solución de código abierto disponible para el reconocimiento óptico de caracteres.
- Combinado con la librería de procesamiento de imágenes /Leptonica/, lee varios formatos de imagen y convierte texto de más de sesenta idiomas.
- En 2006 Google retoma el proyecto y lo mejora.
- Está disponible para /Windows/, /Mac OSX/ y /GNU/Linux/ en línea de comandos.
- Cuenta con una licencia /Apache License 2.0/.

Para utilizarlo en línea de comandos, podemos hacerlo de esta simple manera:
#+BEGIN_SRC sh
tesseract imagen.png out 
#+END_SRC

Lo que producirá un archivo =out.txt= con el texto que ha conseguido reconocer de esa imagen.

Podemos especificar el idioma con la opción =l=
#+BEGIN_SRC sh
tesseract imagen.png out -l spa
#+END_SRC

Donde =spa= corresponde a /Spanish/. Otros idiomas pueden ser:
- =ara= para árabe
- =cat= para catalán
- =chi_sim= para chino simplificado
- =chi_tra= para chino tradicional
- =deu= para alemán
- =eng= para inglés
- =fra= para francés
- =glg= para gallego
- =ita= para italiano
- =rus= para ruso.

El listado completo lo puedes encontrar en https://tesseract-ocr.googlecode.com/svn/trunk/doc/tesseract.1.html#_languages
- Las instrucciones para la instalación están disponibles en https://code.google.com/p/tesseract-ocr/wiki/ReadMe.
- El manual completo con las distintas opciones y argumentos de entrada y salida está disponible en https://tesseract-ocr.googlecode.com/svn/trunk/doc/tesseract.1.html
- Tesseract-ocr, https://code.google.com/p/tesseract-ocr/

* Referencias bibliográficas						 :OK:

- Aristarain, Manuel & Tigas, Mike & Merril, Jeremy B. (2014) /Scraping PDFs with Tabula/. URL: https://s3.amazonaws.com/media.miketigas.com/files/20140627/20140627-tabula-IRE2014-withnotes.pdf

- Crucianelli, Sandra. (2013) /Herramientas digitales para periodistas/. Centro Knight para el Periodismo en las Américas de la Universidad de la Universidad de Texas. URL: https://knightcenter.utexas.edu/books/HDPP.pdf

- García Santiago, Lola. (2003) /Extraer y visualizar información en Internet: el Web Mining/. Gijón: Ediciones Trea

- Gray, Jonathan & Bounegru, Liliana & Chambers, Lucy. (2012) /Data Journalism Handbook/. European Journalism Centre y Open Knowledge Foundation. URL: http://datajournalismhandbook.org/

- Kayser-Brill, Nicolas. (2014) /Data wants to be free! (and clean)/. Medialab-Prado. URL: http://bit.ly/free-clean

- Méndez Rodriguez, Eva Mª. (2002) /Metadatos y Recuperación de información: estándares, problemas y aplicabilidad en bibliotecas digitales/. Gijón: Trea

- Nguyen, Dan. (2010) /Chapter 3: Turning PDFs to Text/. Propublica, Journalism in the Public Interest. URL: https://www.propublica.org/nerds/item/turning-pdfs-to-text-doc-dollars-guide

- Nguyen, Dan. (2010) /Chapter 5: Getting Text Out of an Image-Only PDF/. ProPublica, Journalism in the Public Interest. URL: https://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick

- Schoolofdata, (2014) /Obteniendo datos de los PDF/. Web: School of Data. URL: http://es.schoolofdata.org/obteniendo-datos-de-los-pdfs/




* Manuales
- Cómo utilizar /Google OCR/,  https://www.youtube.com/watch?v=DPJJON26Do4
- Introducción al scraping de /PDF/, http://www.irekia.euskadi.eus/es/news/11703-introduccion-google-refine-curso-periodismo-datos




